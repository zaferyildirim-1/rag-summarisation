{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxOaSxtJWV1G"
   },
   "source": [
    "# WELCOME\n",
    "\n",
    "This notebook will guide you through two increasingly significant applications in the realm of Generative AI: RAG (Retrieval Augmented Generation) chatbots and text summarization for big text.\n",
    "\n",
    "Through two distinct projects, you will explore these technologies and enhance your skills. Detailed descriptions of the projects are provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9GmKlL2NRff"
   },
   "outputs": [],
   "source": [
    "## Project 2: Generating PDF Document Summaries\n",
    "\n",
    "In this project, you will explore various methods for creating summaries from the provided PDF document. You will experiment with different chaining functions offered by the Langchain library to achieve this.\n",
    "\n",
    "### **Project Steps:**\n",
    "- **1.PDF Document Upload and Chunking:** As in the first project, upload the PDF document and divide it into smaller chunks. Consider splitting it by half-page or page.\n",
    "\n",
    "- **2.Summarization Techniques:**\n",
    "\n",
    "  - **Summary of the First 5 Pages (Stuff Chain):** Utilize the load_summarize_chain function with the parameter chain_type=\"stuff\" to generate a concise summary of the first 5 pages of the PDF document.\n",
    "\n",
    "  - **Short Summary of the Entire Document (Map Reduce Chain):** Employ chain_type=\"map_reduce\" and refine parameters to create a brief summary of the entire document. This method generates individual summaries for each chunk and then combines them into a final summary.\n",
    "\n",
    "  - **Detailed Summary with Bullet Points (Map Reduce Chain):** Use chain_type=\"map_reduce\" to generate a detailed summary with at least 1000 tokens. Provide the LLM with the prompt \"Summarize with 1000 tokens\" and set the max_token parameter to a value greater than 1000. Add a title to the summary and present key points using bullet points.\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "- Models like GPT-4o and Gemini Pro models might excel in generating summaries based on token count. Consider prioritizing these models.\n",
    "\n",
    "- For comprehensive information on Langchain and LLMs, refer to their respective documentation.\n",
    "Best of luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ClhhFbDAUGU"
   },
   "outputs": [],
   "source": [
    "#.       1- first 5 pages with stuff chain\n",
    "#.       2- entire doc with map reduce\n",
    "#.       3- entire doc with map reduce and summarise 1000 tokens, max_token 1000+ + title and bullet points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhjLe0IqRnl4"
   },
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "ZXdV8CcqbFrW"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain-community\n",
    "!pip install -qU langchain-openai\n",
    "\n",
    "!pip install -qU pypdfium2       # handling and parsing PDF files\n",
    "#LLM app (as shown in the diagram) to ingest a common and important type of proprietary data: PDF documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "deC1xYujI2bK"
   },
   "outputs": [],
   "source": [
    "# OpenAI API key as an environment variable within the Google Colab notebook.\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqImlx_IRqQS"
   },
   "source": [
    "### Loading PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "CCkT3msfbH_n"
   },
   "outputs": [],
   "source": [
    "# retrieving unstructured data (text from a PDF) and preparing it for the LLM to process.\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "def read_doc(directory):\n",
    "    file_loader=PyPDFium2Loader(directory)\n",
    "    pdf_documents=file_loader.load()\n",
    "    return pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a_FpBOcbHzP",
    "outputId": "d52e7ee1-d732-4d39-c419-6cbc1a6890cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pdf = read_doc('/content/drive/MyDrive/nlp/bert_article.pdf')\n",
    "\n",
    "len(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r87LK6sZMJrC",
    "outputId": "f5908e5d-baf3-4a6a-c963-599091b54adb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-29T17:36:03+00:00', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'subject': 'N19-1 2019', 'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'source': '/content/drive/MyDrive/nlp/bert_article.pdf', 'total_pages': 16, 'page': 15}, page_content='4186\\nC Additional Ablation Studies\\nC.1 Effect of Number of Training Steps\\nFigure 5 presents MNLI Dev accuracy after fine\\x02tuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh fine-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How\\x02ever, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\nC.2 Ablation for Different Masking\\nProcedures\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\nNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\n200 400 600 800 1,000\\n76\\n78\\n80\\n82\\n84\\nPre-training Steps (Thousands)\\nMNLI Dev Accuracy\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after fine-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.\\nand fine-tuning, as the [MASK] symbol never ap\\x02pears during the fine-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both fine-tuning and feature-based ap\\x02proaches, as we expect the mismatch will be am\\x02plified for the feature-based approach as the model\\nwill not have the chance to adjust the representa\\x02tions.\\nMasking Rates Dev Set Results\\nMASK SAME RND MNLI NER\\nFine-tune Fine-tune Feature-based\\n80% 10% 10% 84.2 95.4 94.9\\n100% 0% 0% 84.3 94.9 94.0\\n80% 0% 20% 84.1 95.2 94.6\\n80% 20% 0% 84.4 95.2 94.7\\n0% 20% 80% 83.7 94.8 94.6\\n0% 0% 100% 83.6 94.9 94.6\\nTable 8: Ablation over different masking strategies.\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\nThe numbers in the left part of the table repre\\x02sent the probabilities of the specific strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\nFrom the table it can be seen that fine-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat\\x02egy was problematic when applying the feature\\x02based approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf[-1:]  # first page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuyT0IoWR4n8"
   },
   "source": [
    "### Summarizing the First 5 Pages of The Document With Chain_Type of The 'stuff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "O3yAnW3PbKIX"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# Set up the LLM\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,              # deterministic output\n",
    "    model_name='gpt-4o-mini',   # compact and powerful\n",
    "    max_tokens=2048             # control summary length\n",
    ")\n",
    "\n",
    "# Create the summarization chain using \"stuff\"\n",
    "chain_stuff = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "# Select only the first 5 pages of the document\n",
    "first_5_pages = pdf[:5]\n",
    "\n",
    "try:\n",
    "    summary_stuff = chain_stuff.invoke(first_5_pages)  # summarise only 5 pages\n",
    "    Markdown(\"Summary of the First 5 Pages (Stuff Chain):\")\n",
    "    (summary_stuff[\"output_text\"])\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8wopgGPibKA3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "89fe54c2-1353-4a1a-dc88-5aed213c0e30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-29T17:36:03+00:00', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'subject': 'N19-1 2019', 'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'source': '/content/drive/MyDrive/nlp/bert_article.pdf', 'total_pages': 16, 'page': 0}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\nc 2019 Association for Computational Linguistics\\n4171\\nBERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\nAbstract\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re\\x02lationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce fine-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\nThere are two existing strategies for apply\\x02ing pre-trained language representations to down\\x02stream tasks: feature-based and fine-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-specific architectures that\\ninclude the pre-trained representations as addi\\x02tional features. The fine-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-specific parameters, and is trained on the\\ndownstream tasks by simply fine-tuning all pre\\x02trained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe\\x02cially for the fine-tuning approaches. The ma\\x02jor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi\\x02tectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\nIn this paper, we improve the fine-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n'),\n",
       "  Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-29T17:36:03+00:00', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'subject': 'N19-1 2019', 'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'source': '/content/drive/MyDrive/nlp/bert_article.pdf', 'total_pages': 16, 'page': 1}, page_content='4172\\nword based only on its context. Unlike left-to\\x02right language model pre-training, the MLM ob\\x02jective enables the representation to fuse the left\\nand the right context, which allows us to pre\\x02train a deep bidirectional Transformer. In addi\\x02tion to the masked language model, we also use\\na “next sentence prediction” task that jointly pre\\x02trains text-pair representations. The contributions\\nof our paper are as follows:\\n• We demonstrate the importance of bidirectional\\npre-training for language representations. Un\\x02like Radford et al. (2018), which uses unidirec\\x02tional language models for pre-training, BERT\\nuses masked language models to enable pre\\x02trained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task\\x02specific architectures. BERT is the first fine\\x02tuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper\\x02forming many task-specific architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod\\x02els are available at https://github.com/\\ngoogle-research/bert.\\n2 Related Work\\nThere is a long history of pre-training general lan\\x02guage representations, and we briefly review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems, of\\x02fering significant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre\\x02train word embedding vectors, left-to-right lan\\x02guage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis\\x02criminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed\\x02dings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen\\x02tence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto\\x02encoder derived objectives (Hill et al., 2016).\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re\\x02search along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep\\x02resentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-specific architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques\\x02tion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre\\x02dict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod\\x02els.\\n2.2 Unsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the first\\nworks in this direction only pre-trained word em\\x02bedding parameters from unlabeled text (Col\\x02lobert and Weston, 2008).\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nfine-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre\\x02viously state-of-the-art results on many sentence\\x02level tasks from the GLUE benchmark (Wang\\net al., 2018a). Left-to-right language model-\\n'),\n",
       "  Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-29T17:36:03+00:00', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'subject': 'N19-1 2019', 'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'source': '/content/drive/MyDrive/nlp/bert_article.pdf', 'total_pages': 16, 'page': 2}, page_content='4173\\nBERT BERT\\nE[CLS] E1\\n E[SEP] ... EN\\nE1’ ... EM’\\nC T1 T[SEP] ... TN\\nT1’ ... TM’\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1\\n E[SEP] ... EN\\nE1’ ... EM’\\nC T1 T[SEP] ... TN\\nT1’ ... TM’\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\nMasked Sentence A Masked Sentence B\\nPre-training Fine-Tuning\\nNSP Mask LM Mask LM\\nUnlabeled Sentence A and B Pair \\nSQuAD\\nQuestion Answer Pair\\nMNLI NER\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n2.3 Transfer Learning from Supervised Data\\nThere has also been work showing effective trans\\x02fer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon\\x02strated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to fine-tune models pre-trained with Ima\\x02geNet (Deng et al., 2009; Yosinski et al., 2014).\\n3 BERT\\nWe introduce BERT and its detailed implementa\\x02tion in this section. There are two steps in our\\nframework: pre-training and fine-tuning. Dur\\x02ing pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For fine\\x02tuning, the BERT model is first initialized with\\nthe pre-trained parameters, and all of the param\\x02eters are fine-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep\\x02arate fine-tuned models, even though they are ini\\x02tialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its unified ar\\x02chitecture across different tasks. There is mini\\x02mal difference between the pre-trained architec\\x02ture and the final downstream architecture.\\nModel Architecture BERT’s model architec\\x02ture is a multi-layer bidirectional Transformer en\\x02coder based on the original implementation de\\x02scribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im\\x02plementation is almost identical to the original,\\nwe will omit an exhaustive background descrip\\x02tion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.\\n3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param\\x02eters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans\\x02former uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1\\nhttps://github.com/tensorflow/tensor2tensor\\n2\\nhttp://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3\\nIn all cases we set the feed-forward/filter size to be 4H,\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n4We note that in the literature the bidirectional Trans-\\n'),\n",
       "  Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-29T17:36:03+00:00', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'subject': 'N19-1 2019', 'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'source': '/content/drive/MyDrive/nlp/bert_article.pdf', 'total_pages': 16, 'page': 3}, page_content='4174\\nInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., h Question, Answeri) in one token sequence.\\nThroughout this work, a “sentence” can be an arbi\\x02trary span of contiguous text, rather than an actual\\nlinguistic sentence. A “sequence” refers to the in\\x02put token sequence to BERT, which may be a sin\\x02gle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The first\\ntoken of every sequence is always a special clas\\x02sification token ([CLS]). The final hidden state\\ncorresponding to this token is used as the ag\\x02gregate sequence representation for classification\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed\\x02ding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the final hidden\\nvector of the special [CLS] token as C ∈ R\\nH,\\nand the final hidden vector for the i\\nth input token\\nas Ti ∈ R\\nH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza\\x02tion of this construction can be seen in Figure 2.\\n3.1 Pre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper\\x02vised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason\\x02able to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to\\x02right and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec\\x02tional conditioning would allow each word to in\\x02directly “see itself”, and the model could trivially\\npredict the target word in a multi-layered context.\\nformer is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.\\nIn order to train a deep bidirectional representa\\x02tion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a “masked\\nLM” (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the final hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to\\x02kens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon\\x02structing the entire input.\\nAlthough this allows us to obtain a bidirec\\x02tional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and\\nfine-tuning, since the [MASK] token does not ap\\x02pear during fine-tuning. To mitigate this, we do\\nnot always replace “masked” words with the ac\\x02tual [MASK] token. The training data generator\\nchooses 15% of the token positions at random for\\nprediction. If the i-th token is chosen, we replace\\nthe i-th token with (1) the [MASK] token 80% of\\nthe time (2) a random token 10% of the time (3)\\nthe unchanged i-th token 10% of the time. Then,\\nTi will be used to predict the original token with\\ncross entropy loss. We compare variations of this\\nprocedure in Appendix C.2.\\nTask #2: Next Sentence Prediction (NSP)\\nMany important downstream tasks such as Ques\\x02tion Answering (QA) and Natural Language Infer\\x02ence (NLI) are based on understanding the rela\\x02tionship between two sentences, which is not di\\x02rectly captured by language modeling. In order\\nto train a model that understands sentence rela\\x02tionships, we pre-train for a binarized next sen\\x02tence prediction task that can be trivially gener\\x02ated from any monolingual corpus. Specifically,\\nwhen choosing the sentences A and B for each pre\\x02training example, 50% of the time B is the actual\\nnext sentence that follows A (labeled as IsNext),\\nand 50% of the time it is a random sentence from\\nthe corpus (labeled as NotNext). As we show\\nin Figure 1, C is used for next sentence predic\\x02tion (NSP).5 Despite its simplicity, we demon\\x02strate in Section 5.1 that pre-training towards this\\ntask is very beneficial to both QA and NLI. 6\\n5The final model achieves 97%-98% accuracy on NSP.\\n6The vector C is not a meaningful sentence representation\\nwithout fine-tuning, since it was trained with NSP.\\n'),\n",
       "  Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-29T17:36:03+00:00', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'subject': 'N19-1 2019', 'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'source': '/content/drive/MyDrive/nlp/bert_article.pdf', 'total_pages': 16, 'page': 4}, page_content='4175\\n[CLS] my dog is cute [SEP] he likes play ##ing [SEP] Input\\nE[CLS] Ehe Elikes Eplay E##ing E[SEP] Emy Edog Eis Ecute E[SEP]\\nToken\\nEmbeddings\\nEAEBEBEBEBEBEAEAEAEAEA\\nSegment\\nEmbeddings\\nE0 E6E7E8E9E10 E1E2E3E4E5\\nPosition\\nEmbeddings\\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta\\x02tion embeddings and the position embeddings.\\nThe NSP task is closely related to representation\\x02learning objectives used in Jernite et al. (2017) and\\nLogeswaran and Lee (2018). However, in prior\\nwork, only sentence embeddings are transferred to\\ndown-stream tasks, where BERT transfers all pa\\x02rameters to initialize end-task model parameters.\\nPre-training data The pre-training procedure\\nlargely follows the existing literature on language\\nmodel pre-training. For the pre-training corpus we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages\\nand ignore lists, tables, and headers. It is criti\\x02cal to use a document-level corpus rather than a\\nshuffled sentence-level corpus such as the Billion\\nWord Benchmark (Chelba et al., 2013) in order to\\nextract long contiguous sequences.\\n3.2 Fine-tuning BERT\\nFine-tuning is straightforward since the self\\x02attention mechanism in the Transformer al\\x02lows BERT to model many downstream tasks—\\nwhether they involve single text or text pairs—by\\nswapping out the appropriate inputs and outputs.\\nFor applications involving text pairs, a common\\npattern is to independently encode text pairs be\\x02fore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text\\npair with self-attention effectively includes bidi\\x02rectional cross attention between two sentences.\\nFor each task, we simply plug in the task\\x02specific inputs and outputs into BERT and fine\\x02tune all the parameters end-to-end. At the in\\x02put, sentence A and sentence B from pre-training\\nare analogous to (1) sentence pairs in paraphras\\x02ing, (2) hypothesis-premise pairs in entailment, (3)\\nquestion-passage pairs in question answering, and\\n(4) a degenerate text-∅ pair in text classification\\nor sequence tagging. At the output, the token rep\\x02resentations are fed into an output layer for token\\x02level tasks, such as sequence tagging or question\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classification, such as en\\x02tailment or sentiment analysis.\\nCompared to pre-training, fine-tuning is rela\\x02tively inexpensive. All of the results in the pa\\x02per can be replicated in at most 1 hour on a sin\\x02gle Cloud TPU, or a few hours on a GPU, starting\\nfrom the exact same pre-trained model.7 We de\\x02scribe the task-specific details in the correspond\\x02ing subsections of Section 4. More details can be\\nfound in Appendix A.5.\\n4 Experiments\\nIn this section, we present BERT fine-tuning re\\x02sults on 11 NLP tasks.\\n4.1 GLUE\\nThe General Language Understanding Evaluation\\n(GLUE) benchmark (Wang et al., 2018a) is a col\\x02lection of diverse natural language understanding\\ntasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo fine-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R\\nH corresponding to the first\\ninput token ([CLS]) as the aggregate representa\\x02tion. The only new parameters introduced during\\nfine-tuning are classification layer weights W ∈\\nR\\nK×H, where K is the number of labels. We com\\x02pute a standard classification loss with C and W,\\ni.e., log(softmax(CWT)).\\n7\\nFor example, the BERT SQuAD model can be trained in\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\nF1 score of 91.0%.\\n8\\nSee (10) in https://gluebenchmark.com/faq.\\n')],\n",
       " 'output_text': \"The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. BERT pre-trains deep bidirectional representations from unlabeled text by jointly considering both left and right contexts, overcoming limitations of previous unidirectional models. It employs a masked language model (MLM) and a next sentence prediction (NSP) task during pre-training, allowing it to achieve state-of-the-art results on eleven natural language processing tasks, including question answering and language inference. BERT's architecture is unified across tasks, requiring minimal task-specific modifications during fine-tuning. The model demonstrates significant improvements over existing approaches, achieving notable performance metrics on benchmarks like GLUE and SQuAD. The code and pre-trained models are publicly available for further research and application.\"}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "collapsed": true,
    "id": "dMl-n5AlNx4L",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "39fec6c9-c2e5-4499-ac32-7c90950da53b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. BERT pre-trains deep bidirectional representations from unlabeled text by jointly considering both left and right contexts, overcoming limitations of previous unidirectional models. It employs a masked language model (MLM) and a next sentence prediction (NSP) task during pre-training, allowing it to achieve state-of-the-art results on eleven natural language processing tasks, including question answering and language inference. BERT's architecture is unified across tasks, requiring minimal task-specific modifications during fine-tuning. The model demonstrates significant improvements over existing approaches, achieving notable performance metrics on benchmarks like GLUE and SQuAD. The code and pre-trained models are publicly available for further research and application.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_stuff[\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "id": "xeC07EY8k1dB",
    "outputId": "1e7e9831-1443-4dd7-b53b-1d577896de50"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. BERT pre-trains deep bidirectional representations from unlabeled text by jointly considering both left and right contexts, overcoming limitations of previous unidirectional models. It employs a masked language model (MLM) and a next sentence prediction (NSP) task during pre-training, allowing it to achieve state-of-the-art results on eleven natural language processing tasks, including question answering and language inference. BERT's architecture is unified across tasks, requiring minimal task-specific modifications during fine-tuning. The model demonstrates significant improvements over existing approaches, achieving notable performance metrics on benchmarks like GLUE and SQuAD. The code and pre-trained models are publicly available for further research and application."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The output is a dictionary, extract and display the final summary\n",
    "Markdown(summary_stuff[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RuXzQl9cldD4",
    "outputId": "8df32044-50dd-4389-dfd3-dd00a5c397f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7e260c6cb7a0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7e260c6c9310>, root_client=<openai.OpenAI object at 0x7e260d54c650>, root_async_client=<openai.AsyncOpenAI object at 0x7e260c6c8fb0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), max_tokens=2048), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='text')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We are now directly modifying the instruction that is sent to the language model.\n",
    "\n",
    "chain_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5t3RF5oCldBY",
    "outputId": "961c204b-d5e2-4b23-f94c-14c959d200a3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ilk bes sayfayi ozette bunu kullanalim bi tik iyi olsun\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chain_stuff.llm_chain.prompt.template # models template is this.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "WvZA9JeJlc_d"
   },
   "outputs": [],
   "source": [
    "#  replacing the default prompt with a custom one that gives the LLM specific instructions.\n",
    "\n",
    "#By including \"Write a summary in 1000 tokens...\", we are providing explicit guidance to the model about the desired length and style of the summary.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "F6i_jIzjlc8u"
   },
   "outputs": [],
   "source": [
    "#chain_stuff.llm_chain.prompt.template = \"\"\"Write a summary in 750 tokens of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:\"\"\"\n",
    "\n",
    "# Refine the summarisation prompt for scientific text\n",
    "chain_stuff.llm_chain.prompt.template = \"\"\"Write a summary of the following text in about 750 tokens.\n",
    "- Focus on the main contributions, methods, datasets, and results.\n",
    "- Do not introduce information not present in the text.\n",
    "- Keep the language objective and concise.\n",
    "- Preserve key technical terms and abbreviations.\n",
    "\n",
    "Text:\n",
    "\"{text}\"\n",
    "\n",
    "CONCISE SUMMARY:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rVAzFZKlc6f",
    "outputId": "0bf3549f-0809-4771-fc6d-f5bce1f70a05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Write a summary of the following text in about 750 tokens.\\n- Focus on the main contributions, methods, datasets, and results.\\n- Do not introduce information not present in the text.\\n- Keep the language objective and concise.\\n- Preserve key technical terms and abbreviations.\\n\\nText:\\n\"{text}\"\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7afd9f3aacf0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7afd9a4a6c90>, root_client=<openai.OpenAI object at 0x7afd9fc363c0>, root_async_client=<openai.AsyncOpenAI object at 0x7afdaf3ff470>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), max_tokens=2048), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='text')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt.template has changed now.\n",
    "\n",
    "chain_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ORbJ18rFlc38",
    "outputId": "5176abd1-6ead-4cd2-a1b1-b3cee3a0426f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Write a summary of the following text in about 750 tokens.\\n- Focus on the main contributions, methods, datasets, and results.\\n- Do not introduce information not present in the text.\\n- Keep the language objective and concise.\\n- Preserve key technical terms and abbreviations.\\n\\nText:\\n\"{text}\"\\n\\nCONCISE SUMMARY:'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_stuff.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "a3knDSSjlc1M"
   },
   "outputs": [],
   "source": [
    "#  the effect of this change, we will need to run the chain again with the new prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "kfLMLvohl-ng",
    "outputId": "e7d69a7b-a8bb-4569-b8b0-4a0e3e4a88d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=c5a74ced-8d93-44da-8890-e2a0524198a1,id=c5a74ced-8d93-44da-8890-e2a0524198a1; trace=c5a74ced-8d93-44da-8890-e2a0524198a1,id=23a8c4a0-d70f-4b81-a541-4940fec84c11; trace=c5a74ced-8d93-44da-8890-e2a0524198a1,id=ea438bc8-8909-46d0-a1c2-a6ac7cdf1524\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. BERT\\'s primary innovation lies in its ability to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context, which contrasts with previous models that utilized unidirectional approaches. This bidirectionality allows BERT to achieve state-of-the-art performance across a variety of natural language processing (NLP) tasks with minimal task-specific architecture modifications.\\n\\nBERT employs two main pre-training tasks: a masked language model (MLM) and next sentence prediction (NSP). The MLM randomly masks a percentage of input tokens and trains the model to predict these masked tokens based solely on their context, enabling the model to learn bidirectional representations. The NSP task involves predicting whether a given sentence follows another in the text, which is crucial for tasks that require understanding relationships between sentences.\\n\\nThe model architecture is based on a multi-layer bidirectional Transformer encoder, with two primary configurations: BERTBASE (12 layers, 110 million parameters) and BERTLARGE (24 layers, 340 million parameters). BERT\\'s input representation can handle both single sentences and pairs of sentences, utilizing special tokens to differentiate between them.\\n\\nBERT was pre-trained on a large corpus comprising the BooksCorpus (800 million words) and English Wikipedia (2.5 billion words). The fine-tuning process is straightforward, allowing BERT to adapt to various downstream tasks by simply adding a task-specific output layer and fine-tuning all parameters.\\n\\nThe results demonstrate BERT\\'s effectiveness, achieving new state-of-the-art scores on eleven NLP tasks, including a GLUE score of 80.5% (an improvement of 7.7 percentage points), MultiNLI accuracy of 86.7% (4.6 points improvement), and SQuAD v1.1 F1 score of 93.2 (1.5 points improvement) and SQuAD v2.0 F1 score of 83.1 (5.1 points improvement). BERT\\'s performance significantly surpasses that of previous models, including OpenAI GPT, particularly on tasks with limited training data.\\n\\nThe paper also discusses the advantages of BERT\\'s bidirectional pre-training over existing unidirectional models, highlighting its ability to reduce the need for complex task-specific architectures. The authors provide extensive experimental results, ablation studies, and comparisons with other models, demonstrating the robustness and versatility of BERT across various NLP applications. The code and pre-trained models are made publicly available, facilitating further research and application in the field.'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0,\n",
    "                 model_name='gpt-4o-mini',\n",
    "                 max_tokens=1024)\n",
    "\n",
    "chain_stuff = load_summarize_chain(\n",
    "                      llm,\n",
    "                      chain_type='stuff'\n",
    "                      )\n",
    "\n",
    "summary_stuff = chain_stuff.invoke(pdf)['output_text']\n",
    "summary_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "W29bOQFZl-h6",
    "outputId": "6ee0c029-d9ad-4626-a742-aefd28adae88"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model developed by Google AI Language. BERT's primary innovation lies in its ability to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right contexts across all layers. This approach allows BERT to achieve state-of-the-art performance on various natural language processing (NLP) tasks with minimal task-specific architecture modifications.\n",
       "\n",
       "### Main Contributions:\n",
       "1. **Bidirectional Pre-training**: BERT utilizes a masked language model (MLM) pre-training objective, which randomly masks tokens in the input and predicts them based on their context, enabling deep bidirectional representations. This contrasts with previous models that employed unidirectional training.\n",
       "2. **Reduction of Task-Specific Architecture**: BERT demonstrates that pre-trained representations can significantly reduce the need for complex task-specific architectures, achieving state-of-the-art results across eleven NLP tasks.\n",
       "3. **Performance Improvements**: BERT sets new records on multiple benchmarks, including achieving an 80.5% score on the GLUE benchmark, 86.7% accuracy on MultiNLI, and 93.2 F1 on SQuAD v1.1.\n",
       "\n",
       "### Methods:\n",
       "BERT's framework consists of two main steps: pre-training and fine-tuning. During pre-training, BERT is trained on unlabeled data using two tasks:\n",
       "- **Masked Language Model (MLM)**: Randomly masks 15% of the input tokens and predicts them based on their context.\n",
       "- **Next Sentence Prediction (NSP)**: Trains the model to predict whether a second sentence follows the first in the text.\n",
       "\n",
       "The model architecture is based on a multi-layer bidirectional Transformer encoder, with two primary configurations: BERTBASE (12 layers, 110M parameters) and BERTLARGE (24 layers, 340M parameters).\n",
       "\n",
       "### Datasets:\n",
       "BERT is pre-trained on a large corpus comprising BooksCorpus (800M words) and English Wikipedia (2,500M words). For fine-tuning, BERT is evaluated on various NLP tasks, including:\n",
       "- GLUE benchmark tasks (e.g., MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, RTE)\n",
       "- SQuAD v1.1 and v2.0 for question answering\n",
       "- SWAG for grounded common-sense inference\n",
       "\n",
       "### Results:\n",
       "BERT achieves significant improvements over previous state-of-the-art models:\n",
       "- **GLUE Benchmark**: BERTBASE and BERTLARGE outperform all prior systems, with BERTLARGE achieving an average accuracy of 82.1%.\n",
       "- **SQuAD v1.1**: BERTLARGE achieves a test F1 score of 93.2, surpassing previous top systems.\n",
       "- **SQuAD v2.0**: BERTLARGE achieves a test F1 score of 83.1, marking a 5.1 point improvement over the previous best.\n",
       "- **SWAG**: BERTLARGE achieves an accuracy of 86.6, outperforming existing models by a substantial margin.\n",
       "\n",
       "### Conclusion:\n",
       "BERT's introduction of bidirectional pre-training and its effective use of the MLM and NSP tasks represent a significant advancement in the field of NLP. The model's ability to generalize across various tasks with minimal architecture changes highlights the potential of deep bidirectional architectures in language understanding. The code and pre-trained models are publicly available, facilitating further research and application in the NLP community."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output is a dictionary, extract and display the final summary\n",
    "\n",
    "Markdown(summary_stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ev879-3hl-dP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BV25O1flcZn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvrLsoivTulb"
   },
   "source": [
    "### Document Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LakeybChjrGb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "5j9NMbSCbMyf"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# for gpt-4o-mini 128,000-token context window ... text in each \"map\" step.\n",
    "\n",
    "# Create an intelligent text splitter that attempts to split on different characters (\\n\\n, \\n, ) in a recursive order to keep related sentences and paragraphs together.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000,\n",
    "                                               chunk_overlap=0,  # ensures there is no overlapping text between the chunks.\n",
    "                                               )\n",
    "\n",
    "# Run the chain on the newly created chunks\n",
    "chunks = text_splitter.split_documents(pdf)\n",
    "\n",
    "# The chunks variable now contains a list of these smaller document pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53mpwb7KbMrf",
    "outputId": "84254ea1-8c41-440a-a7fb-0bfbfbf78a57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1XgTRfiimYf",
    "outputId": "0eb1850e-aad7-4817-f9b3-c86e1ba66159"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-29T17:36:03+00:00', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'subject': 'N19-1 2019', 'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'source': '/content/drive/MyDrive/nlp/bert_article.pdf', 'total_pages': 16, 'page': 0}, page_content='Proceedings of NAACL-HLT 2019, pages 4171–4186\\nMinneapolis, Minnesota, June 2 - June 7, 2019. \\nc 2019 Association for Computational Linguistics\\n4171\\nBERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\nAbstract\\nWe introduce a new language representa\\x02tion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre\\x02sentation models (Peters et al., 2018a; Rad\\x02ford et al., 2018), BERT is designed to pre\\x02train deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re\\x02sult, the pre-trained BERT model can be fine\\x02tuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re\\x02lationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce fine-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\nThere are two existing strategies for apply\\x02ing pre-trained language representations to down\\x02stream tasks: feature-based and fine-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-specific architectures that\\ninclude the pre-trained representations as addi\\x02tional features. The fine-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-specific parameters, and is trained on the\\ndownstream tasks by simply fine-tuning all pre\\x02trained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe\\x02cially for the fine-tuning approaches. The ma\\x02jor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi\\x02tectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to\\x02right architecture, where every token can only at\\x02tend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re\\x02strictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying fine\\x02tuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor\\x02porate context from both directions.\\nIn this paper, we improve the fine-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi\\x02rectionality constraint by using a “masked lan\\x02guage model” (MLM) pre-training objective, in\\x02spired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "K47pfk6rjDki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "3OymwEupjGIX",
    "outputId": "ba32bc89-58b7-4eed-b071-b3e608dd95f4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a revolutionary language representation model by Google AI Language that employs a deep bidirectional architecture for pre-training on unlabeled text. This approach allows BERT to effectively fine-tune for various NLP tasks, achieving state-of-the-art results on eleven benchmarks, including GLUE and SQuAD, and surpassing previous models like OpenAI GPT. BERT's pre-training involves Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), enhancing its contextual understanding. The model comes in two sizes, BERTBASE and BERTLARGE, and shows significant performance improvements, especially with limited training data. The paper also reviews advancements in NLP, comparing BERT with other models, detailing its pre-training and fine-tuning processes, and emphasizing the impact of masking strategies on performance. Overall, BERT marks a significant advancement in NLP, showcasing the benefits of bidirectional context.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zlVe2iISX0Q"
   },
   "source": [
    "--\n",
    "-\n",
    "--\n",
    "-\n",
    "\n",
    "\n",
    "# Make A Brief Summary of The Entire Document With Chain_Types of \"map_reduce\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH1H7QdXobcx"
   },
   "source": [
    "###When to Use\n",
    "\n",
    "map_reduce → best for speed + large docs (100s of pages), when high-level overview is fine.\n",
    "\n",
    "refine → best for accuracy & continuity (academic papers, technical reports), when detail matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "FaEjU0Mym08U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "t6wPW3OFbOcJ",
    "outputId": "735a71ea-33f6-430f-f4dc-b7974cca7daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 101 ms, sys: 5.72 ms, total: 107 ms\n",
      "Wall time: 35.3 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"The paper presents BERT (Bidirectional Encoder Representations from Transformers), a groundbreaking language representation model developed by Google AI Language. Unlike previous unidirectional models, BERT utilizes a masked language model (MLM) and next sentence prediction (NSP) for pre-training, allowing it to consider both left and right contexts, which significantly enhances its performance across eleven natural language processing (NLP) tasks. BERT achieves state-of-the-art results on benchmarks like GLUE and SQuAD, demonstrating its effectiveness with minimal task-specific modifications. The model's architecture, characterized by a multi-layer bidirectional Transformer encoder, facilitates easy fine-tuning for various applications. The paper also discusses the impact of model size and pre-training tasks on performance, highlighting the importance of bidirectionality and the effectiveness of rich, unsupervised pre-training. BERT's results surpass those of previous models, showcasing its potential in advancing NLP capabilities.\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Brief Summary of The Entire Document With Chain_Types of \"map_reduc\n",
    "\n",
    "\n",
    "%%time\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000,\n",
    "                                               chunk_overlap=0,  # ensures there is no overlapping text between the chunks.\n",
    "                                               )\n",
    "\n",
    "# Run the chain on the newly created chunks\n",
    "chunks = text_splitter.split_documents(pdf)\n",
    "\n",
    "# The chunks variable now contains a list of these smaller document pieces.\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=2500)\n",
    "\n",
    "# Optional: stronger scientific prompts (map/combine) can be passed via chain_type_kwargs\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    return_intermediate_steps=True,  # helpful to inspect per-chunk outputs\n",
    "    # chain_type_kwargs={\"map_prompt\": map_prompt, \"combine_prompt\": combine_prompt}\n",
    ")\n",
    "\n",
    "result = chain.invoke(chunks)\n",
    "final_summary = result[\"output_text\"]\n",
    "intermediate = result[\"intermediate_steps\"]  # if you want to inspect/debug\n",
    "\n",
    "final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "collapsed": true,
    "id": "8NpJYGxRbOVC",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c1ba8b81-1bdb-4b91-c00b-870c8bcd33eb"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The paper presents BERT (Bidirectional Encoder Representations from Transformers), a groundbreaking language representation model developed by Google AI Language. Unlike previous unidirectional models, BERT utilizes a masked language model (MLM) and next sentence prediction (NSP) for pre-training, allowing it to consider both left and right contexts, which significantly enhances its performance across eleven natural language processing (NLP) tasks. BERT achieves state-of-the-art results on benchmarks like GLUE and SQuAD, demonstrating its effectiveness with minimal task-specific modifications. The model's architecture, characterized by a multi-layer bidirectional Transformer encoder, facilitates easy fine-tuning for various applications. The paper also discusses the impact of model size and pre-training tasks on performance, highlighting the importance of bidirectionality and the effectiveness of rich, unsupervised pre-training. BERT's results surpass those of previous models, showcasing its potential in advancing NLP capabilities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intermediate\n",
    "from IPython.display import Markdown\n",
    "Markdown(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy-6sm3PpdIx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qlj7-_ExprJN"
   },
   "outputs": [],
   "source": [
    "# a custom PromptTemplate for the 'map' step of the Map-Reduce chain\n",
    "\n",
    "# Purpose: Its sole job is to instruct the LLM on ****how to summarize each individual chunk**** of the original document.\n",
    "\n",
    "# Input: It takes the text of a ***single document chunk *** (e.g., one page of your PDF) as its input.\n",
    "# Goal: To generate a concise, intermediate summary for that specific chunk. This result will be fed into the next step. The instructions are simple because the task is limited to a single piece of text.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 789
    },
    "id": "zajOKo3rbzcd",
    "outputId": "92b7c416-82da-41d4-a9cc-85f96d412459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 ms, sys: 6.15 ms, total: 114 ms\n",
      "Wall time: 44.6 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Title: Overview of BERT: Advancements in Natural Language Processing**\n",
       "\n",
       "**Summary:**\n",
       "\n",
       "The text provides a comprehensive overview of BERT (Bidirectional Encoder Representations from Transformers), a groundbreaking language representation model developed by Google AI Language. BERT's innovative approach to pre-training and fine-tuning has significantly advanced the field of natural language processing (NLP). The following points summarize the key aspects discussed in the text:\n",
       "\n",
       "1. **Introduction to BERT**: BERT is designed to pre-train deep bidirectional representations from unlabeled text, considering both left and right context, which enhances its performance on various NLP tasks.\n",
       "\n",
       "2. **Performance on NLP Tasks**: BERT achieves state-of-the-art results on eleven tasks, including GLUE, MultiNLI, and SQuAD benchmarks, demonstrating significant improvements over previous models.\n",
       "\n",
       "3. **Bidirectional Pre-training**: The model employs a masked language model (MLM) approach, allowing for better context incorporation compared to unidirectional models, which enhances its effectiveness in sentence-level and token-level tasks.\n",
       "\n",
       "4. **Unified Architecture**: BERT utilizes a unified architecture for various tasks, with minimal differences between pre-training and downstream models, facilitating efficient fine-tuning.\n",
       "\n",
       "5. **Input Representations**: BERT combines token, segment, and position embeddings to create input representations, using WordPiece embeddings and special tokens for classification and sentence separation.\n",
       "\n",
       "6. **Pre-training and Fine-tuning**: The pre-training phase involves tasks like MLM and Next Sentence Prediction (NSP), while fine-tuning adjusts the model using labeled data for specific tasks, allowing for quick adaptation.\n",
       "\n",
       "7. **Model Sizes**: BERT is available in two main sizes: BERTBASE and BERTLARGE, with the latter showing superior performance, especially on tasks with limited training data.\n",
       "\n",
       "8. **Benchmark Results**: BERT models significantly outperform previous state-of-the-art models on the GLUE benchmark and SQuAD v1.1, achieving notable accuracy improvements.\n",
       "\n",
       "9. **Ablation Studies**: The text discusses various ablation studies that highlight the importance of pre-training tasks and model size, indicating that larger models consistently improve accuracy across tasks.\n",
       "\n",
       "10. **Impact of Pre-training**: The findings emphasize the significance of rich, unsupervised pre-training in enhancing language understanding systems, particularly for low-resource tasks.\n",
       "\n",
       "11. **Comparative Analysis**: BERT's architecture is compared with other models like OpenAI GPT and ELMo, showcasing its unique bidirectional approach and fine-tuning capabilities.\n",
       "\n",
       "12. **Hyperparameter Tuning**: The text discusses the importance of hyperparameter tuning during fine-tuning, noting that larger datasets are less sensitive to hyperparameter choices.\n",
       "\n",
       "13. **Task-Specific Fine-tuning**: Various tasks for fine-tuning BERT are outlined, including linguistic acceptability, semantic similarity, and paraphrase detection, with suggestions for multitask approaches to enhance performance.\n",
       "\n",
       "14. **Ongoing Research**: The text concludes with mentions of ongoing ablation studies to further understand the importance of different aspects of the BERT model and its applications in NLP.\n",
       "\n",
       "This summary encapsulates the essential points regarding BERT's architecture, training processes, performance metrics, and its transformative impact on natural language processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map reduce in one block.  . next one not this one.\n",
    "#-----------------------------------\n",
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "# prompt for every chunk\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Define our custom prompt. Prompt for combining the summaries\n",
    "chunks_prompt = \"\"\"\n",
    "                Please summarize the below text:\n",
    "                text:'{text}'\n",
    "                summary:\n",
    "                \"\"\"\n",
    "\n",
    "map_prompt_template = PromptTemplate(input_variables=['text'],\n",
    "                                     template=chunks_prompt)\n",
    "\n",
    "# prompt for combined summaries\n",
    "#all the smaller summaries  >>>>>. coherent, cohesive, and well-structured final summary.\n",
    "#.   map ----------------->.         reduce\n",
    "\n",
    "final_combine_prompt = \"\"\"\n",
    "                       Provide a final summary of the entire text with important points.\n",
    "                       Add a Generic  Title,\n",
    "                       Start the precise summary with an introduction and provide the\n",
    "                       summary in number points for the text.\n",
    "                       text: '{text}'\n",
    "                       summary:\n",
    "                       \"\"\"\n",
    "\n",
    "final_combine_prompt_template = PromptTemplate(input_variables=['text'],\n",
    "                                               template=final_combine_prompt)\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000,\n",
    "                                               chunk_overlap=0,  # ensures there is no overlapping text between the chunks.\n",
    "                                               )\n",
    "\n",
    "# Run the chain on the newly created chunks\n",
    "chunks = text_splitter.split_documents(pdf)\n",
    "#llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, max_tokens=1600)\n",
    "\n",
    "\n",
    "chain = load_summarize_chain(llm=llm,\n",
    "                             chain_type='map_reduce',\n",
    "                             map_prompt=map_prompt_template,  # every chunk\n",
    "                             combine_prompt=final_combine_prompt_template  # combined all summarization\n",
    "                             )\n",
    "\n",
    "chain\n",
    "\n",
    "# result = chain.invoke(chunks)\n",
    "# final_summary = result[\"output_text\"]\n",
    "# # intermediate = result[\"intermediate_steps\"]  # if you want to inspect/debug\n",
    "# final_summary\n",
    "\n",
    "\n",
    "\n",
    "output_summary = chain.invoke(chunks)[\"output_text\"]\n",
    "\n",
    "#output_summary\n",
    "\n",
    "# The output is a dictionary, extract and display the final summary\n",
    "Markdown(output_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfxPVkrlshoi"
   },
   "source": [
    "The following prompt, final_combine_prompt (The 'Reduce' Prompt), is used for the second and final step.\n",
    "\n",
    "Purpose: Its job is to synthesize all the individual summaries from the first step into one single, final summary.\n",
    "Input: It takes the collection of all the smaller summaries (the output of the 'map' step) as its input.\n",
    "Goal: To create a coherent, cohesive, and well-structured final summary. This prompt is where we add instructions for formatting, style, and content that should apply to the final output, such as adding a title, an introduction, and using numbered points.\n",
    "In essence, the map prompt handles the summarization of parts, while the reduce prompt handles the synthesis of the whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk-_WD7Bswvb"
   },
   "source": [
    "We have now successfully created a Map-Reduce summarization chain using our custom prompts for both the 'map' and 'reduce' steps.\n",
    "\n",
    "The output from chain will display the structure of this new MapReduceDocumentsChain, confirming that it has been correctly configured with our specific map_prompt_template and final_combine_prompt_template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPMe4ABTnsR5",
    "outputId": "4c88dc77-deb2-43c3-915c-60e085154c90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNA199vwh0ln"
   },
   "outputs": [],
   "source": [
    "serial operating barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "W9qsC19jwxO7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "6b9f2e72-291a-4831-8d63-9cf9627abe42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 160 ms, sys: 8.44 ms, total: 168 ms\n",
      "Wall time: 1min 35s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**BERT: Bidirectional Encoder Representations from Transformers**\n",
       "\n",
       "- BERT (Bidirectional Encoder Representations from Transformers) is a revolutionary language representation model created by Google AI Language, aimed at improving natural language processing (NLP) tasks.\n",
       "- The model utilizes deep bidirectional representations, allowing it to consider both left and right context simultaneously, leading to a more nuanced understanding of language.\n",
       "- BERT can be fine-tuned for various NLP tasks by adding a simple output layer, achieving state-of-the-art results without complex architectural changes.\n",
       "- It sets new benchmarks across eleven NLP tasks, including:\n",
       "  - GLUE score: 80.5% (an absolute improvement of 7.7%)\n",
       "  - MultiNLI accuracy: 86.7% (4.6% improvement)\n",
       "  - SQuAD v1.1 F1 score: 93.2 (1.5 point improvement)\n",
       "  - SQuAD v2.0 F1 score: 83.1 (5.1 point improvement)\n",
       "- The pre-training phase of BERT significantly boosts performance in both sentence-level and token-level tasks, overcoming limitations of prior unidirectional models.\n",
       "- BERT employs a masked language model (MLM) objective, which enhances context understanding by randomly masking input tokens and predicting them.\n",
       "- The model also incorporates a \"next sentence prediction\" (NSP) task, which aids in understanding relationships between sentences, essential for tasks like Question Answering (QA) and Natural Language Inference (NLI).\n",
       "- BERT's architecture is based on a multi-layer bidirectional Transformer encoder, available in two configurations: BERTBASE (12 layers, 110M parameters) and BERTLARGE (24 layers, 340M parameters).\n",
       "- Input representation in BERT accommodates both single and paired sentences, utilizing WordPiece embeddings and special tokens ([CLS] for classification and [SEP] for sentence separation).\n",
       "- Pre-training is conducted on extensive corpora, including BooksCorpus and English Wikipedia, focusing on document-level sequences to enhance long-range dependencies.\n",
       "- Fine-tuning is straightforward, leveraging the self-attention mechanism to adapt the model for specific tasks with minimal adjustments.\n",
       "- BERT's performance on the GLUE benchmark showcases its superiority over previous systems, with BERTBASE achieving an average accuracy of 79.6% and BERTLARGE reaching 82.1%.\n",
       "- In the SQuAD v1.1 dataset, BERT surpasses existing systems, achieving an F1 score of 91.8 in ensemble settings and outperforming top leaderboard systems.\n",
       "- For SQuAD v2.0, BERT's approach includes treating questions without answers as spans at the [CLS] token, enhancing its ability to manage unanswerable questions.\n",
       "- BERT excels on the SWAG dataset, outperforming previous models by significant margins.\n",
       "- Fine-tuning details include specific epochs, learning rates, and batch sizes tailored for each task, ensuring optimal performance.\n",
       "- Ablation studies indicate the critical role of pre-training tasks, revealing that removing NSP significantly degrades performance on key tasks.\n",
       "- Larger model sizes correlate with improved accuracy, with BERTLARGE consistently outperforming smaller configurations across various tasks.\n",
       "- The study emphasizes the effectiveness of transfer learning with language models, particularly in low-resource scenarios, and highlights the advantages of deep bidirectional architectures.\n",
       "- BERT's fine-tuning method, which involves adding a classification layer and jointly fine-tuning all parameters, proves more effective than traditional feature-based approaches.\n",
       "- Comparisons with other models, such as OpenAI GPT, illustrate BERT's superior performance, especially in tasks with limited training data.\n",
       "- The research underscores BERT's potential to advance the state of the art in NLP, paving the way for future developments in language representation and understanding.\n",
       "- BERT's pre-training involves two main tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP), enhancing its contextual understanding and sentence relationship comprehension.\n",
       "- The MLM task randomly masks 15% of input tokens, employing a specific replacement strategy to promote contextual learning.\n",
       "- The NSP task samples pairs of sentences, with half being consecutive, helping the model learn inter-sentence relationships.\n",
       "- BERT's architecture includes special tokens like [CLS] for classification and [SEP] for separating sentences, improving its versatility across various NLP tasks.\n",
       "- The model is trained with a maximum sequence length of 512 tokens and employs a batch size of 256 sequences over approximately 1,000,000 steps, utilizing the Adam optimizer with specific hyperparameters.\n",
       "- BERT's pre-training is computationally intensive, with BERTBASE taking 4 days on 4 Cloud TPUs and BERTLARGE on 16 Cloud TPUs.\n",
       "- Fine-tuning BERT for specific tasks involves adding an output layer and adjusting hyperparameters like learning rate and batch size, with optimal batch sizes ranging from 16 to 32.\n",
       "- BERT's performance is evaluated on the GLUE benchmark, which includes various tasks such as MNLI (entailment classification), QQP (question pair classification), and SST-2 (sentiment analysis).\n",
       "- The model's fine-tuning is sensitive to hyperparameters, particularly on smaller datasets, while large datasets (100k+ labeled examples) show less sensitivity.\n",
       "- BERT's training and fine-tuning processes are compared to other models like ELMo and OpenAI GPT, highlighting differences in architecture, training data, and learning strategies.\n",
       "- Ablation studies reveal that BERT's improvements stem primarily from its pre-training tasks and bidirectionality, with mixed masking strategies during MLM pre-training yielding better results than single strategies.\n",
       "- The model's performance on tasks like MNLI shows that increasing the number of training steps leads to higher accuracy, with BERTBASE achieving nearly 1.0% higher accuracy with 1M steps compared to 500k.\n",
       "- BERT's architecture and training methods have set a new standard in NLP, demonstrating the effectiveness of deep contextualized representations and bidirectional training in understanding language."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "# --- Map-Reduce: Detailed, ≥1000 tokens, Title + bullet points ---\n",
    "\n",
    "\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# 1) Map prompt (per chunk) – keep concise bullets so reduce can scale\n",
    "map_prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the following text as concise bullet points (no intro):\\n\\n{text}\\n\\n-\"\n",
    ")\n",
    "\n",
    "# 2) Combine prompt – EXACT requirement text + structure\n",
    "combine_prompt = PromptTemplate.from_template(\n",
    "    \"Summarize with around 1000 tokens. Produce a detailed, cohesive summary of the ENTIRE work.\\n\"\n",
    "    \"- Start with a single (**bold**) Generic Title on the first line (no quotes).\\n\"\n",
    "    \"- Then present the key points as bullet points only (no numbering).\\n\"\n",
    "    \"- Be comprehensive and avoid redundancy; prefer clear, information-dense bullets.\\n\\n\"\n",
    "    \"All chunk summaries:\\n{text}\\n\\n\"\n",
    "    \"Title:\\n\"\n",
    "    \"Bullet-point summary:\\n-\"\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=8000,\n",
    "                                               chunk_overlap=0,  # ensures there is no overlapping text between the chunks.\n",
    "                                               )\n",
    "\n",
    "# Run the chain on the newly created chunks\n",
    "chunks = text_splitter.split_documents(pdf)\n",
    "\n",
    "# 3) LLM with max_tokens > 1000\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=1234)\n",
    "\n",
    "# 4) Build chain\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    #verbose=True\n",
    ")\n",
    "\n",
    "# 5) Run\n",
    "result = chain.invoke(chunks)  # 'chunks' = list[Document]\n",
    "output_summary = result[\"output_text\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Markdown(output_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N9RjtNfUhZPi",
    "outputId": "80b0c64b-4e75-4ef0-9934-77fe4180fd24"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**BERT: Bidirectional Encoder Representations from Transformers**\n",
       "\n",
       "- BERT (Bidirectional Encoder Representations from Transformers) is a revolutionary language representation model created by Google AI Language, aimed at improving natural language processing (NLP) tasks.\n",
       "- The model utilizes deep bidirectional representations, considering both left and right context, which enhances its understanding of language nuances.\n",
       "- BERT can be easily fine-tuned for various NLP tasks by adding a simple output layer, achieving state-of-the-art results without complex architectural changes.\n",
       "- It has set new performance benchmarks on eleven NLP tasks, including:\n",
       "  - GLUE score: 80.5% (an absolute improvement of 7.7%)\n",
       "  - MultiNLI accuracy: 86.7% (an absolute improvement of 4.6%)\n",
       "  - SQuAD v1.1 F1 score: 93.2 (an absolute improvement of 1.5 points)\n",
       "  - SQuAD v2.0 F1 score: 83.1 (an absolute improvement of 5.1 points)\n",
       "- Traditional pre-training methods using unidirectional language models are limited; BERT's masked language model (MLM) pre-training addresses these limitations, improving context integration for tasks like question answering.\n",
       "- The MLM enables BERT to predict masked tokens in sentences, leveraging both left and right context, which is essential for effective language representation.\n",
       "- BERT incorporates a \"next sentence prediction\" (NSP) task, which helps the model understand relationships between sentence pairs during pre-training.\n",
       "- The architecture is based on a multi-layer bidirectional Transformer encoder, available in two configurations: BERTBASE (12 layers, 110M parameters) and BERTLARGE (24 layers, 340M parameters).\n",
       "- BERT's input representation supports both single and paired sentences, using WordPiece embeddings with a vocabulary of 30,000 tokens.\n",
       "- The first token in the input sequence is a special classification token ([CLS]), whose final hidden state is used for classification tasks, while sentence pairs are separated by a special token ([SEP]).\n",
       "- Pre-training is conducted on large corpora, including BooksCorpus (800M words) and English Wikipedia (2,500M words), focusing on document-level contexts for long sequences.\n",
       "- Fine-tuning is efficient, utilizing the self-attention mechanism to model various tasks with text pairs, and can be executed on Cloud TPU or GPU.\n",
       "- BERT's fine-tuning results show significant improvements across multiple NLP tasks, with BERTBASE and BERTLARGE outperforming all previous systems on the GLUE benchmark.\n",
       "- In SQuAD v1.1, BERTLARGE achieves an F1 score of 91.8, surpassing the leading system by 1.5 points, while in SQuAD v2.0, it outperforms existing systems by 0.1-0.4 F1.\n",
       "- The SWAG dataset results indicate BERTLARGE achieving an EM score of 86.6, outperforming previous models by substantial margins.\n",
       "- Ablation studies highlight the importance of various components of BERT, showing that removing the NSP task significantly reduces performance on several tasks.\n",
       "- Larger BERT models consistently yield better accuracy across tasks, with BERTLARGE demonstrating notable improvements even with limited training data.\n",
       "- The study emphasizes the effectiveness of fine-tuning models on downstream tasks, suggesting that larger pre-trained representations can enhance performance, even for low-resource tasks.\n",
       "- BERT's performance is compared with other models, including ELMo and OpenAI GPT, showcasing its superior capabilities across various benchmarks.\n",
       "- The research underscores the advantages of transfer learning with language models, which boosts performance across diverse NLP tasks.\n",
       "- BERT's configurations were tested with varying layers, hidden sizes, and attention heads, revealing that larger models lead to significant performance gains.\n",
       "- The findings suggest that while feature-based approaches have their merits, BERT's fine-tuning method provides a more effective solution for many NLP challenges.\n",
       "- The paper concludes that BERT represents a significant advancement in NLP, offering a robust framework for future research and applications in language understanding.\n",
       "- The work also discusses various advances in NLP, including datasets like TriviaQA for reading comprehension, frameworks like Skip-thought vectors for sentence representation, and benchmarks like the Winograd Schema Challenge for commonsense reasoning.\n",
       "- BERT's architecture is detailed in three sections: implementation details, experimental details, and ablation studies, providing a comprehensive understanding of its design and performance.\n",
       "- Pre-training tasks for BERT include a masked language model (MLM) with specific masking strategies to encourage robust contextual representations.\n",
       "- The masking procedure involves replacing words with [MASK], random words, or keeping them unchanged, promoting diverse learning during training.\n",
       "- BERT employs a bidirectional Transformer architecture, contrasting with OpenAI GPT's left-to-right architecture, enhancing its contextual understanding.\n",
       "- BERT's representations are conditioned on both left and right context, allowing for a more nuanced understanding of language compared to feature-based models like ELMo.\n",
       "- BERT's MLM converges slower than left-to-right models but shows significant empirical improvements in performance.\n",
       "- The Next Sentence Prediction (NSP) task involves sampling text spans to assess the model's understanding of sentence relationships.\n",
       "- Training sequences are limited to 512 tokens, with a 15% masking rate applied after WordPiece tokenization, optimizing input for the model.\n",
       "- BERT is trained with a batch size of 256 sequences over 1,000,000 steps, utilizing the Adam optimizer with specific hyperparameters for effective learning.\n",
       "- Fine-tuning BERT involves adjusting hyperparameters for specific tasks, with optimal batch sizes ranging from 16 to 32.\n",
       "- The comparison of BERT, ELMo, and OpenAI GPT highlights differences in training data, architecture, and learning strategies, emphasizing BERT's design for comprehensive language understanding.\n",
       "- Ablation studies reveal that improvements in performance are primarily due to pre-training tasks and the bidirectional nature of BERT.\n",
       "- The GLUE benchmark includes various tasks such as MNLI, QQP,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6prTVyn8wxAG"
   },
   "outputs": [],
   "source": [
    "\n",
    "bi alltaki kodun bitmesi cok uzun sureceginden burada hata olsun ki zaman kaybi olmasin\n",
    ".line 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zYsGX3rBmv_A",
    "outputId": "9bb5d14a-bcb4-4181-a667-8b84372a0641"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# BERT: A Breakthrough in Language Representation Models\n",
       "\n",
       "This paper presents BERT, a novel language representation model that enhances natural language processing (NLP) tasks by utilizing deep bidirectional representations from unlabeled text.\n",
       "\n",
       "1. **Introduction of BERT**: BERT stands for Bidirectional Encoder Representations from Transformers and is designed to pre-train deep bidirectional representations by conditioning on both left and right context in all layers. Unlike previous models that used unidirectional language models, BERT employs a masked language model (MLM) objective, which allows it to fuse left and right context effectively.\n",
       "\n",
       "2. **Input/Output Representations**: BERT's input representation can unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer) in one token sequence. A \"sequence\" refers to the input token sequence, which may consist of a single sentence or two sentences packed together. The first token of every sequence is a special classification token ([CLS]), and the final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are differentiated using a special token ([SEP]) and a learned embedding indicating whether a token belongs to sentence A or B. The input embeddings are the sum of the token embeddings, segment embeddings, and position embeddings.\n",
       "\n",
       "3. **Pre-training Method**: BERT employs a masked language model (MLM) pre-training objective, which randomly masks tokens in the input and predicts their original values, overcoming the limitations of unidirectional models. The masking procedure involves replacing a token with the [MASK] token 80% of the time, a random word 10% of the time, and leaving it unchanged 10% of the time. Additionally, BERT incorporates a \"next sentence prediction\" (NSP) task that jointly pre-trains text-pair representations. The model is trained on a large corpus, including BooksCorpus (800M words) and English Wikipedia (2,500M words), using a unified architecture that remains consistent during fine-tuning. Studies show that removing the NSP task significantly degrades performance on tasks like QNLI, MNLI, and SQuAD 1.1. Recent ablation studies indicate that BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained for 1 million steps compared to 500,000 steps, highlighting the importance of extensive pre-training.\n",
       "\n",
       "4. **Fine-tuning Capability**: The pre-trained BERT model can be fine-tuned with a single additional output layer, allowing it to achieve state-of-the-art performance across various NLP tasks without significant architectural changes. Fine-tuning is typically very fast, and it is reasonable to run an exhaustive search over hyperparameters such as learning rates (5e-5, 3e-5, 2e-5) and the number of epochs (2, 3, 4) to choose the best-performing model on the development set. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. BERT is the first fine-tuning based representation model that outperforms many task-specific architectures.\n",
       "\n",
       "5. **Feature-based Approach**: While BERT's results primarily utilize a fine-tuning approach, there are advantages to a feature-based approach where fixed features are extracted from the pre-trained model. This method can be computationally efficient, allowing for the pre-computation of expensive representations of training data, which can then be used with simpler models. Experiments on tasks like Named Entity Recognition (NER) demonstrate that BERT can effectively support both fine-tuning and feature-based methods, with competitive performance across both approaches. However, ablation studies show that fine-tuning is robust to different masking strategies, while the feature-based approach struggles when using only the [MASK] strategy.\n",
       "\n",
       "6. **Model Architecture**: BERT’s architecture is a multi-layer bidirectional Transformer encoder, based on the original implementation described in Vaswani et al. (2017). It primarily reports results on two model sizes: BERTBASE (12 layers, 768 hidden size, 12 attention heads, 110M parameters) and BERTLARGE (24 layers, 1024 hidden size, 16 attention heads, 340M parameters). The use of bidirectional self-attention distinguishes BERT from models like OpenAI GPT, which uses constrained self-attention. BERT's representations are jointly conditioned on both left and right context in all layers, unlike ELMo, which uses a concatenation of independently trained left-to-right and right-to-left LSTMs.\n",
       "\n",
       "7. **Performance Improvements**: BERT achieves new state-of-the-art results on eleven NLP tasks, including:\n",
       "   - GLUE score: 80.5% (7.7% absolute improvement)\n",
       "   - MultiNLI accuracy: 86.7% (4.6% absolute improvement)\n",
       "   - SQuAD v1.1 F1 score: 93.2 (1.5 point absolute improvement)\n",
       "   - SQuAD v2.0 F1 score: 83.1 (5.1 point absolute improvement)\n",
       "   - BERTBASE and BERTLARGE outperform all systems on all tasks by substantial margins, with average accuracy improvements of 4.5% and 7.0% respectively over the prior state of the art. Notably, BERTLARGE (Ensemble) achieved an F1 score of 91.8 on SQuAD 1.1, outperforming other top systems.\n",
       "\n",
       "8. **Comparison with Existing Models**: BERT improves upon previous models by addressing the limitations of unidirectional language models, which restrict the effectiveness of fine-tuning approaches, particularly for token-level tasks like question answering. It demonstrates the importance of bidirectional pre-training for language representations, contrasting with models that use shallow concatenation of independently trained left-to-right and right-to-left language models. BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach. Notably, BERT's training involved a larger dataset and different hyperparameter choices compared to GPT, which contributes to its performance advantages.\n",
       "\n",
       "9. **Implications for NLP**: The introduction of BERT signifies a shift in how language representations can be utilized, enhancing the performance of various NLP applications and setting a new benchmark for future research. The code and pre-trained models are available at https://github.com/google-research/bert. Additionally, the findings indicate that larger models lead to improved accuracy across tasks, demonstrating the effectiveness of scaling model size in NLP applications. Recent empirical improvements suggest that even low-resource tasks can benefit from deep bidirectional architectures, further generalizing the advantages of rich, unsupervised pre-training in language understanding systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# refine in one block\n",
    "\n",
    "# prgoressive approach, after each chunks summarisation it refine itself\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "\n",
    "# (Use the same llm you used above, or uncomment the next 2 lines)\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=1600)\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# --- initial prompt (first chunk) ---\n",
    "initial_prompt_txt = \"\"\"\n",
    "You are creating a structured summary of the text below.\n",
    "\n",
    "Requirements:\n",
    "- Add a short, generic Title on the first line (no quotes).\n",
    "- Then write a concise introduction (1–2 sentences).\n",
    "- Follow with a numbered list of key points (prioritise salient facts, methods, results, implications).\n",
    "- Keep the whole summary precise and cohesive.\n",
    "\n",
    "text: '{text}'\n",
    "summary:\n",
    "\"\"\"\n",
    "\n",
    "initial_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=initial_prompt_txt\n",
    ")\n",
    "\n",
    "# --- refine prompt (subsequent chunks) ---\n",
    "refine_prompt_txt = \"\"\"\n",
    "We have an existing structured summary that should be improved using NEW text.\n",
    "\n",
    "Requirements:\n",
    "- Preserve the structure: Title, brief introduction, then numbered key points.\n",
    "- Integrate any new important information; merge or renumber points as needed.\n",
    "- Remove duplication, keep coherence and concise phrasing.\n",
    "- If the new text adds nothing important, keep the current summary unchanged.\n",
    "- The final output must remain a single summary (Title + intro + numbered points).\n",
    "\n",
    "Existing summary:\n",
    "{existing_answer}\n",
    "\n",
    "New text:\n",
    "{text}\n",
    "\n",
    "Refined summary:\n",
    "\"\"\"\n",
    "\n",
    "refine_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_prompt_txt\n",
    ")\n",
    "\n",
    "# --- build chain ---\n",
    "refine_chain = load_summarize_chain(\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=initial_prompt_template,   # first chunk\n",
    "    refine_prompt=refine_prompt_template,      # subsequent chunks\n",
    ")\n",
    "\n",
    "# --- run ---\n",
    "# 'chunks' should be a list of langchain Document objects (same as in your map_reduce block)\n",
    "output_summary = refine_chain.invoke(chunks)[\"output_text\"]\n",
    "\n",
    "# --- display ---\n",
    "Markdown(output_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIvi7EcKep_P"
   },
   "source": [
    "BERT: A Breakthrough in Language Representation Models\n",
    "This paper presents BERT, a novel language representation model that enhances natural language processing (NLP) tasks by utilizing deep bidirectional representations from unlabeled text.\n",
    "\n",
    "Introduction of BERT: BERT stands for Bidirectional Encoder Representations from Transformers and is designed to pre-train deep bidirectional representations by conditioning on both left and right context in all layers. Unlike previous models that used unidirectional language models, BERT employs a masked language model (MLM) objective, which allows it to fuse left and right context effectively.\n",
    "\n",
    "Input/Output Representations: BERT's input representation can unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer) in one token sequence. A \"sequence\" refers to the input token sequence, which may consist of a single sentence or two sentences packed together. The first token of every sequence is a special classification token ([CLS]), and the final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are differentiated using a special token ([SEP]) and a learned embedding indicating whether a token belongs to sentence A or B. The input embeddings are the sum of the token embeddings, segment embeddings, and position embeddings.\n",
    "\n",
    "Pre-training Method: BERT employs a masked language model (MLM) pre-training objective, which randomly masks tokens in the input and predicts their original values, overcoming the limitations of unidirectional models. The masking procedure involves replacing a token with the [MASK] token 80% of the time, a random word 10% of the time, and leaving it unchanged 10% of the time. Additionally, BERT incorporates a \"next sentence prediction\" (NSP) task that jointly pre-trains text-pair representations. The model is trained on a large corpus, including BooksCorpus (800M words) and English Wikipedia (2,500M words), using a unified architecture that remains consistent during fine-tuning. Studies show that removing the NSP task significantly degrades performance on tasks like QNLI, MNLI, and SQuAD 1.1. Recent ablation studies indicate that BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained for 1 million steps compared to 500,000 steps, highlighting the importance of extensive pre-training.\n",
    "\n",
    "Fine-tuning Capability: The pre-trained BERT model can be fine-tuned with a single additional output layer, allowing it to achieve state-of-the-art performance across various NLP tasks without significant architectural changes. Fine-tuning is typically very fast, and it is reasonable to run an exhaustive search over hyperparameters such as learning rates (5e-5, 3e-5, 2e-5) and the number of epochs (2, 3, 4) to choose the best-performing model on the development set. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. BERT is the first fine-tuning based representation model that outperforms many task-specific architectures.\n",
    "\n",
    "Feature-based Approach: While BERT's results primarily utilize a fine-tuning approach, there are advantages to a feature-based approach where fixed features are extracted from the pre-trained model. This method can be computationally efficient, allowing for the pre-computation of expensive representations of training data, which can then be used with simpler models. Experiments on tasks like Named Entity Recognition (NER) demonstrate that BERT can effectively support both fine-tuning and feature-based methods, with competitive performance across both approaches. However, ablation studies show that fine-tuning is robust to different masking strategies, while the feature-based approach struggles when using only the [MASK] strategy.\n",
    "\n",
    "Model Architecture: BERT’s architecture is a multi-layer bidirectional Transformer encoder, based on the original implementation described in Vaswani et al. (2017). It primarily reports results on two model sizes: BERTBASE (12 layers, 768 hidden size, 12 attention heads, 110M parameters) and BERTLARGE (24 layers, 1024 hidden size, 16 attention heads, 340M parameters). The use of bidirectional self-attention distinguishes BERT from models like OpenAI GPT, which uses constrained self-attention. BERT's representations are jointly conditioned on both left and right context in all layers, unlike ELMo, which uses a concatenation of independently trained left-to-right and right-to-left LSTMs.\n",
    "\n",
    "Performance Improvements: BERT achieves new state-of-the-art results on eleven NLP tasks, including:\n",
    "\n",
    "GLUE score: 80.5% (7.7% absolute improvement)\n",
    "MultiNLI accuracy: 86.7% (4.6% absolute improvement)\n",
    "SQuAD v1.1 F1 score: 93.2 (1.5 point absolute improvement)\n",
    "SQuAD v2.0 F1 score: 83.1 (5.1 point absolute improvement)\n",
    "BERTBASE and BERTLARGE outperform all systems on all tasks by substantial margins, with average accuracy improvements of 4.5% and 7.0% respectively over the prior state of the art. Notably, BERTLARGE (Ensemble) achieved an F1 score of 91.8 on SQuAD 1.1, outperforming other top systems.\n",
    "Comparison with Existing Models: BERT improves upon previous models by addressing the limitations of unidirectional language models, which restrict the effectiveness of fine-tuning approaches, particularly for token-level tasks like question answering. It demonstrates the importance of bidirectional pre-training for language representations, contrasting with models that use shallow concatenation of independently trained left-to-right and right-to-left language models. BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach. Notably, BERT's training involved a larger dataset and different hyperparameter choices compared to GPT, which contributes to its performance advantages.\n",
    "\n",
    "Implications for NLP: The introduction of BERT signifies a shift in how language representations can be utilized, enhancing the performance of various NLP applications and setting a new benchmark for future research. The code and pre-trained models are available at https://github.com/google-research/bert. Additionally, the findings indicate that larger models lead to improved accuracy across tasks, demonstrating the effectiveness of scaling model size in NLP applications. Recent empirical improvements suggest that even low-resource tasks can benefit from deep bidirectional architectures, further generalizing the advantages of rich, unsupervised pre-training in language understanding systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "98jDYrDQkzy3"
   },
   "outputs": [],
   "source": [
    "# Great for long documents where you want a single coherent summary updated chunk-by-chunk.\n",
    "\n",
    "# Typically slower than map_reduce, but can keep better narrative continuity.\n",
    "\n",
    "# Tip: In refine, the placeholders are {text} and {existing_answer} (not {context}), so make sure your prompts use those exact variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmDLu9LKkzwg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0AnkRPkkzt5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_HNeDBxkzpW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJFfYRWPkzmc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zZxse-ZUV3S"
   },
   "source": [
    "### Generate A Detailed Summary of The Entire Document With At Least 1000 Tokens. Also, Add A Title To The Summary And Present Key Points Using Bullet Points With Chain_Type of \"map_reduce\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpuiEedJbQME"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdFF9bOAbQEo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hlf2OMY2-OzB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eevGC_p-OwS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDdF3VaE-Osc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
